{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow DMAE\n",
    "\n",
    "This notebook is an example on how to use the tensorflow implementation of the Dissimilarity Mixture Autoencoder (DMAE).\n",
    "We will show some examples on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Path of the DMAE library\n",
    "import sys\n",
    "# sys.path.append(\"/tf/home/repositorios/DMAE/\") # how to use the code in other location\n",
    "sys.path.append(\"../../\")\n",
    "import DMAE\n",
    "\n",
    "# Setting random seed\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# Imporing some helper functions to visualize and understand the results\n",
    "import vis_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotropicly Distributed Blobs\n",
    "\n",
    "First, we're going to generate isotropic Gaussian blobs with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples to generate\n",
    "n_samples = 3200\n",
    "X, _ = make_blobs(n_samples=n_samples, centers=8, cluster_std=0.5)\n",
    "X = np.float32(X)\n",
    "# Visualizing the generated points\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"k\", alpha=0.4)\n",
    "plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Generated Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "n_clusters = 8\n",
    "# Softmax inverse temperature parameter\n",
    "alpha = 1000\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please choose a dissimilarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "button = widgets.Dropdown(options=['Manhattan', 'Euclidean', 'Minkowski', 'Chebyshev', 'Cosine', 'Correlation'], value='Euclidean',\n",
    "                          description='Dissimilarity:', disabled=False)\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "\n",
    "dis_name = \"Euclidean\"\n",
    "def on_change(val):\n",
    "    if val['type'] == 'change' and val['name'] == 'value':\n",
    "        global dis_name\n",
    "        dis_name = val[\"new\"]\n",
    "button.observe(on_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the order $p$ (only possible if you are using the Minkowski dissimilarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dis_name == \"Minkowski\":\n",
    "    button = widgets.FloatSlider(value=1, min=0.1, max=5, step=0.1, description='Order:',\n",
    "                                 disabled=False, continuous_update=True, orientation='horizontal', readout=True,\n",
    "                                 readout_format='.1f')\n",
    "    output = widgets.Output()\n",
    "    display(button, output)\n",
    "    p = 1\n",
    "    def on_change(val):\n",
    "        with output:\n",
    "            if val['type'] == 'change' and val['name'] == 'value':\n",
    "                global p\n",
    "                p = val[\"new\"]\n",
    "    button.observe(on_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the dissimilarity and the loss according to the selected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using dissimilarity [{dis_name}]\")\n",
    "if dis_name == \"Manhattan\":\n",
    "    dis = DMAE.Dissimilarities.manhattan\n",
    "    dmae_loss = DMAE.Losses.manhattan_loss\n",
    "elif dis_name == \"Euclidean\":\n",
    "    dis = DMAE.Dissimilarities.euclidean\n",
    "    dmae_loss = DMAE.Losses.euclidean_loss\n",
    "elif dis_name == \"Minkowski\":\n",
    "    dis = lambda X, Y, batch_size: DMAE.Dissimilarities.minkowsky(X, Y, p, batch_size)\n",
    "    dmae_loss = lambda X, Y, batch_size: DMAE.Losses.minkowsky_loss(X, Y, p, batch_size)\n",
    "elif dis_name == \"Chebyshev\":\n",
    "    dis = DMAE.Dissimilarities.chebyshev\n",
    "    dmae_loss = DMAE.Losses.chebyshev_loss\n",
    "elif dis_name == \"Cosine\":\n",
    "    dis = DMAE.Dissimilarities.cosine\n",
    "    dmae_loss = DMAE.Losses.cosine_loss\n",
    "elif dis_name == \"Correlation\":\n",
    "    dis = DMAE.Dissimilarities.correlation\n",
    "    dmae_loss = DMAE.Losses.correlation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the shallow DMAE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(2, ))\n",
    "# DMM layer\n",
    "theta_tilde = DMAE.Layers.DissimilarityMixtureAutoencoder(alpha=alpha, n_clusters=n_clusters, batch_size=batch_size,\n",
    "                                                          initializers={\"centers\": DMAE.Initializers.InitPlusPlus(X, n_clusters, dis, 100),\n",
    "                                                                        \"mixers\": tf.keras.initializers.Constant(1.0)},\n",
    "                                                          trainable = {\"centers\": True, \"mixers\": True},\n",
    "                                                          dissimilarity=dis)(inp)\n",
    "# DMAE model\n",
    "model = tf.keras.Model(inputs=[inp], outputs=theta_tilde)\n",
    "model.compile(loss=dmae_loss, optimizer=tf.optimizers.Adam(lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, X, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, we define an auxiliar model for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(2,))\n",
    "assigns = DMAE.Layers.DissimilarityMixtureEncoder(alpha=0.1, n_clusters=n_clusters,\n",
    "                                                  dissimilarity=dis,\n",
    "                                                  trainable={\"centers\": False, \"mixers\": False},\n",
    "                                                  batch_size=batch_size)(inp)\n",
    "DMAE_encoder = tf.keras.Model(inputs=[inp], outputs=[assigns])\n",
    "DMAE_encoder.layers[-1].set_weights(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Voronoi regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_regions(DMAE_encoder, dis_name, X, (7, 7), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the mixed distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_distribution(model, dmae_loss, 0.1, X, figsize=(7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior cluster distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_probas(DMAE_encoder, X, n_clusters, rows=2, cols=4, figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Painwheel Data\n",
    "\n",
    "Now, we'll generate the Painwheel synthetic data that was used in: \n",
    "\n",
    "M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, and S. R. Datta, “Composing\n",
    "368 graphical models with neural networks for structured representations and fast inference,” in\n",
    "369 Advances in neural information processing systems, pp. 2946–2954, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_painwheel(rad_std, tan_std, n_groups, N, rate):\n",
    "    \"\"\"\n",
    "    Generates the painwheel data, adapted from: https://github.com/mattjj/svae\n",
    "    Arguments:\n",
    "        rad_std: float\n",
    "            Standard deviation for the angles.\n",
    "        tan_std: float\n",
    "            Tangential standard deviation.\n",
    "        n_groups: int\n",
    "            Number of groups to be generated.\n",
    "        N: int\n",
    "            Total number of samples.\n",
    "        rate: float\n",
    "            Controls the magnitude of the rotations.\n",
    "        \n",
    "    Returns:\n",
    "        D: array-like, shape=(batch_size, n_clusters)\n",
    "            Matrix of paiwise dissimilarities between the batch and the cluster's parameters.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    rads = np.linspace(0, 2*np.pi, n_groups, endpoint=False)\n",
    "    samples = np.random.randn(N, 2)*np.array([rad_std, tan_std])\n",
    "    samples[:,0] += 1\n",
    "    labels = np.repeat(np.arange(n_groups), N//n_groups)\n",
    "\n",
    "    angles = rads[labels]+rate*np.exp(samples[:,0])\n",
    "    rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
    "    rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
    "\n",
    "    return np.random.permutation(np.einsum('ti,tij->tj', samples, rotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_painwheel(0.3, 0.05, 5, 3200, 0.25)\n",
    "# Visualizing the generated points\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.4, c=\"k\")\n",
    "plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Generated Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of prototype vectors\n",
    "n_clusters = 5\n",
    "# Softmax inverse temperature parameter\n",
    "alpha = 1000\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll solve this problem using the Mahalanobis distance (includes the covariances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = DMAE.Dissimilarities.mahalanobis\n",
    "dmae_loss = DMAE.Losses.mahalanobis_loss\n",
    "dis_name = \"Mahalanobis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be pretrained using K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrainer = KMeans(n_clusters=n_clusters).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define a DMAE model for this dissimilarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(2, ))\n",
    "# DMM layer\n",
    "theta_tilde = DMAE.Layers.DissimilarityMixtureAutoencoderCov(alpha=alpha, n_clusters=n_clusters, batch_size=batch_size,\n",
    "                                                             initializers={\"centers\": DMAE.Initializers.InitKMeans(pretrainer),\n",
    "                                                                           \"cov\": DMAE.Initializers.InitKMeansCov(pretrainer, X, n_clusters),\n",
    "                                                                           \"mixers\": tf.keras.initializers.Constant(1.0)},\n",
    "                                                             trainable = {\"centers\": True, \"cov\": True, \"mixers\": True},\n",
    "                                                             dissimilarity=dis)(inp)\n",
    "# DMAE model\n",
    "model = tf.keras.Model(inputs=[inp], outputs=theta_tilde)\n",
    "loss = dmae_loss(inp, *theta_tilde)\n",
    "model.add_loss(loss)\n",
    "model.compile(optimizer=tf.optimizers.Adam(lr=lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results, we define an auxiliar model for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.layers.Input(shape=(2,))\n",
    "assigns = DMAE.Layers.DissimilarityMixtureEncoderCov(alpha=0.1, n_clusters=n_clusters,\n",
    "                                                     dissimilarity=dis,\n",
    "                                                     trainable={\"centers\": False, \"cov\":False, \"mixers\": False},\n",
    "                                                     batch_size=batch_size)(inp)\n",
    "DMAE_encoder = tf.keras.Model(inputs=[inp], outputs=[assigns])\n",
    "DMAE_encoder.layers[-1].set_weights(model.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Voronoi regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_regions(DMAE_encoder, dis_name, X, (7, 7), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the mixed distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_distribution(model, dmae_loss, 0.1, X, figsize=(7, 7), cov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = vis_utils.visualize_probas(DMAE_encoder, X, n_clusters, rows=2, cols=3, figsize=(15, 8))\n",
    "ax[1, 2].axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
